---
title: "Guardrails"
author: Madhav Kadam
date: 2024-04-11
date-modified: 2024-04-11
slug: audit-1
# image: 
categories: 
  - Artificial Intelligence
  - Social Networks
  - Privacy
  - GenAI

# alias:
#   - /blog/2021-a11y-website/01-wave-audit-1/
subtitle: "Are we ready for the future as a human?"
---
The rise of generative AI has ushered in a technological revolution reshaping how we interact with and leverage information. Long before ChatGPT captured global attention, I was fascinated by the immense potential of this field, researching cutting-edge applications like diffusion models for medical imaging. I have seen these technologies employed very innovatively. For example, learning the vocabulary of a new language with AI-curated multimodal content is engaging and enjoyable. Chatbots have become much more efficient in addressing customers' primary concerns. Medical Imaging and Diffusion models are producing results with outstanding accuracy. Many social issues are of grave concern to me and can be solved with enough research and recent advances in Tech put in the right direction, like farmers' distress due to lack of guidance and inequality of opportunity in Education. But this is just the brighter side.

The issue that concerns me the most is the dark patterns in the recommendation algorithms – a problem worsened by the complex generative AI models, the consequences of which I dread. Focused, advanced research in this field is urgently needed. A Netflix documentary "The Social Dilemma" exposed these dangers before the mass proliferation of generative AI, but today's reality is even more alarming. Social networks and online platforms surround our day-to-day lives, serving purposes that range from relationship maintenance and discovering new connections to consuming entertainment and news content and facilitating commerce and self-expression. However, multiple research papers from studies across different nations have established that these very same platforms have also enabled darker use cases – from trolling and abuse to spreading misinformation and propaganda to even the nefarious manipulation of voters.

Technological developments in Recommendation algorithms and mighty powerful GenAI models (trained on the corpus of data with serious privacy concerns) present a double-edged sword. On the one hand, it can make students learn better; on the other, it can even radicalize them. Extensive research is crucial to address these systems' risks to our privacy and autonomy. Recommendation algorithms have resulted in the amplification of extremist content. They are not neutral. After all, recommendation systems are designed to keep us engaged by feeding us more of what we're likely to click on, watch, or read. The more data they collect about our actions, the better they can predict and influence our future behaviour. But the implications go far beyond just shaping what content we see. By analysing small details, these platforms possess the capability to predict our actions, emotions, beliefs and even vulnerability to a surprising level of accuracy. All this deeply personal data exists in corporate databases, largely unprotected from misuse. Brands already leverage these insights to psychologically compel user actions, undergirding the online advertising industry.

Generative models which are trained on web crawls are able to correlate many of these things. By crawling over all the text (and, in recent years, videos and images as well) on the internet, including our social media trails, they can precisely mimic our language, communication styles, and personas. Identity theft has become much easier because of these platforms' lack of new-age security measures. This ability to produce human-like content at scale enables hyper-personalization and targeting that is difficult to distinguish from authentic human engagement. With the powerful multimodal models, these models can also be tweaked
to manufacture very realistic and believable misinformation, fake news, propaganda and armies of bots that can mass manipulate with hyper-personalization. Nation-states and bad actors may leverage this technology for surveillance, social control, and undermining democracy and social cohesion. I have experienced this issue playing a destructive, polarizing role in many events and campaigns. The hate, abuse and trolling one faces on these platforms is simply unbearable for most individuals.

Thus, robust research initiatives are crucially needed across several interrelated domains: human-computer interaction principles, robust authentication mechanisms, ethical web framework designs, unbiased recommendation algorithm development, and the responsible architecture of social networks and generative AI systems. While these endeavours are multifaceted, combining insights from each field can minimize the risks arising from the indiscriminate integration of AI across digital spheres. Ultimately, such collaborative research safeguards authentic human-to-human interactions from corrosive technological influences.

Firstly, robust authentication systems and privacy-preserving approaches must be developed to safeguard user data and autonomy. Distributed systems can potentially mitigate risks of centralized data misuse by making it more difficult for any single entity to conspire. Moreover, establishing clear AI governance frameworks, regulations, and ethical training standards is imperative to ensure generative AI models are developed and deployed responsibly without perpetuating biases or infringing human rights. Recent initiatives like Snowflake's transparent open-sourcing of their Arctic LLM represent a welcomed shift towards responsible AI research in the open. By embracing open-source philosophies and rigorous peer scrutiny, generative model development can become more accountable and aligned with ethical principles from the ground up. Secondly, extensive research is needed in human-computer interaction (HCI) principles and user interface (UI) design. This includes exploring mechanisms to provide users transparency into if/when they are interacting with AI systems, what data is being utilized, and how it influences recommendations or content curation. Interface designs should empower users to audit inferences made about them, object to unfair treatment, and manage their digital footprints consciously. Significant advancements must be made in distinguishing human-generated and AI-generated content across modalities like text, images, and videos. While extremely challenging, developing robust detection algorithms is crucial to combat misinformation, deception, and erosion of trust. Complementing this, Web 4.0 frameworks should incorporate intelligent ways to obfuscate personal data from indiscriminate crawling without user consent.

Without proactive human-centred interventions, we risk ceding our cognitive sovereignty and sleepwalking into a dystopian future of surveillance, manipulation and social fractioning on a societal scale. Maintaining privacy and human agency must be central priorities as generative AI breakthroughs rapidly reshape human-computer interaction.